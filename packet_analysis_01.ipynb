{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from utils.dataframe_tools import filter_out_nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/Test/merge_tls_test_01.csv') \n",
    "df_block = pd.read_csv('./Data/Test/merge_tls_test_01/discrete/0.csv')\n",
    "df_block_tls = pd.read_csv('./Data/Test/merge_tls_test_01/discrete/8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = df.columns.tolist()\n",
    "list_col_block = df_block.columns.tolist()\n",
    "list_col_block_tls = df_block_tls.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fields_by_prefix(layers: list, layer_prefixes: dict, fields: list, len_prefix: int, init: bool): \n",
    "    next_layers = []\n",
    "    if len(layers) > 0: \n",
    "        if init: \n",
    "            for field in fields: \n",
    "                prefix = field.split('.')[0] \n",
    "                if prefix not in layers and len(field.split('.')) - len_prefix == 0: \n",
    "                    layer_prefixes['statistics'].append(field) \n",
    "                elif field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "                    layer_prefixes[prefix].append(field) \n",
    "                    next_layers.append(field) \n",
    "                # for tls, it does not have field like tls.xx, only has tls.xx.xx\n",
    "        else: \n",
    "            for prefix in layers: \n",
    "                # for prefix in layer_prefixes[layer]: \n",
    "                for field in fields: \n",
    "                    if field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "                        if prefix not in layer_prefixes: \n",
    "                            layer_prefixes[prefix] = [] \n",
    "                            layer_prefixes[prefix].append(field) \n",
    "                        else: \n",
    "                            layer_prefixes[prefix].append(field) \n",
    "                    else: \n",
    "                        continue \n",
    "                    next_layers.append(field) \n",
    "            # len_prefix += 1 \n",
    "    init = False\n",
    "    return layer_prefixes, next_layers, init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fields_by_prefix(layers: list, layer_prefixes: dict, fields: list, init: bool): \n",
    "    next_layers = []\n",
    "    if len(layers) > 0: \n",
    "        if init: \n",
    "            for field in fields: \n",
    "                prefix = field.split('.')[0] \n",
    "                len_prefix = len(prefix.split('.')) \n",
    "                if prefix not in layers and len(field.split('.')) - len_prefix == 0: \n",
    "                    layer_prefixes['statistics'].append(field) \n",
    "                elif field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "                    layer_prefixes[prefix].append(field) \n",
    "                    next_layers.append(field) \n",
    "                # for tls, it does not have field like tls.xx, only has tls.xx.xx \n",
    "            empty_layers = [] \n",
    "            extra_len_prefix = len_prefix\n",
    "            # check up the empty layers in initial, and increase the length of prefix. \n",
    "            # We hold an assumption that the emptiness means the length of prefix is not long enough\n",
    "            for key in layer_prefixes.keys(): \n",
    "                if len(layer_prefixes[key]) == 0: \n",
    "                    empty_layers.append(key) \n",
    "            while len(empty_layers) > 0: \n",
    "                extra_len_prefix += 1\n",
    "                for layer in empty_layers[:]: \n",
    "                    for field in fields: \n",
    "                        if field.startswith(layer): \n",
    "                            prefix = '.'.join(field.split('.')[:extra_len_prefix])\n",
    "                            if field.startswith(prefix) and len(field.split('.')) - extra_len_prefix == 1: \n",
    "                                layer_prefixes[layer].append(field) \n",
    "                                next_layers.append(field) \n",
    "                    empty_layers.remove(layer) \n",
    "                    \n",
    "        else: \n",
    "            for prefix in layers: \n",
    "                len_prefix = len(prefix.split('.')) \n",
    "                # for prefix in layer_prefixes[layer]: \n",
    "                for field in fields: \n",
    "                    if field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "                        if prefix not in layer_prefixes: \n",
    "                            layer_prefixes[prefix] = [] \n",
    "                            layer_prefixes[prefix].append(field) \n",
    "                        else: \n",
    "                            layer_prefixes[prefix].append(field) \n",
    "                    else: \n",
    "                        continue \n",
    "                    next_layers.append(field) \n",
    "            # len_prefix += 1 \n",
    "    init = False\n",
    "    return layer_prefixes, next_layers, init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fields_by_prefix_logically(layers: list, layer_prefixes: dict, fields: list, len_prefix: int): \n",
    "    next_layers = [] \n",
    "    if len(layers) > 0: \n",
    "        for field in fields: \n",
    "            prefix = '.'.join(field.split('.')[:len_prefix]) \n",
    "            if prefix not in layers and len(field.split('.')) - len_prefix == 0: \n",
    "                layer_prefixes['statistics'].append(field) \n",
    "            elif field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: # \n",
    "                if prefix not in layer_prefixes: \n",
    "                    layer_prefixes[prefix] = [] \n",
    "                layer_prefixes[prefix].append(field) \n",
    "                next_layers.append(field) \n",
    "            elif field.startswith(prefix) and len(field.split('.')) - len_prefix >= 2: \n",
    "                current_layer = '.'.join(field.split('.')[:len_prefix + 1]) \n",
    "                if prefix in layer_prefixes: \n",
    "                    if current_layer not in layer_prefixes[prefix]: \n",
    "                        layer_prefixes[prefix].append(current_layer) \n",
    "                        next_layers.append(current_layer) \n",
    "                else: # in order to handle the single structure fields like 'tls.ber.bitstring.padding'\n",
    "                    layer_prefixes[prefix] = [] \n",
    "                    layer_prefixes[prefix].append(current_layer) \n",
    "                    next_layers.append(current_layer) \n",
    "    return layer_prefixes, next_layers \n",
    "\n",
    "def protocol_tree(list_fields: list, list_layers = ['eth', 'ip', 'tcp', 'tls']): \n",
    "    \"\"\"\n",
    "    Find the hierarchy structure of protocols by handling the csv columns. \n",
    "    \"\"\"\n",
    "    dict_protocol_tree = {item: [] for item in list_layers} \n",
    "    dict_protocol_tree['statistics'] = [] \n",
    "    lens = [len(item.split('.')) for item in list_fields]\n",
    "    len_prefix = 1 # length of current prefix, i.e. eth \n",
    "    max_field_len = max(lens)\n",
    "    # init = True\n",
    "    while len_prefix < max_field_len: \n",
    "        dict_protocol_tree, list_layers = find_fields_by_prefix_logically(list_layers, dict_protocol_tree, list_fields, len_prefix) \n",
    "        len_prefix += 1\n",
    "    return dict_protocol_tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_tree(list_fields: list, list_layers = ['eth', 'ip', 'tcp', 'tls']): \n",
    "    \"\"\"\n",
    "    Find the hierarchy structure of protocols by handling the csv columns. \n",
    "    \"\"\"\n",
    "    dict_protocol_tree = {item: [] for item in list_layers} \n",
    "    dict_protocol_tree['statistics'] = [] \n",
    "    lens = [len(item.split('.')) for item in list_fields]\n",
    "    len_prefix = 1 # length of current prefix, i.e. eth \n",
    "    max_field_len = max(lens)\n",
    "    init = True\n",
    "    while len_prefix < max_field_len: \n",
    "        dict_protocol_tree, list_layers, init = find_fields_by_prefix(list_layers, dict_protocol_tree, list_fields, init) \n",
    "        len_prefix += 1\n",
    "    return dict_protocol_tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame_num', 'eth.dst', 'eth.dst.lg', 'eth.dst.ig', 'eth.src', 'eth.src.lg', 'eth.src.ig', 'eth.type', 'ip.version', 'ip.hdr_len', 'ip.dsfield', 'ip.dsfield.dscp', 'ip.dsfield.ecn', 'ip.len', 'ip.id', 'ip.flags', 'ip.flags.rb', 'ip.flags.df', 'ip.flags.mf', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport', 'tcp.stream', 'tcp.len', 'tcp.seq', 'tcp.seq_raw', 'tcp.ack', 'tcp.ack_raw', 'tcp.hdr_len', 'tcp.flags', 'tcp.flags.res', 'tcp.flags.ae', 'tcp.flags.cwr', 'tcp.flags.ece', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.flags.str', 'tcp.window_size_value', 'tcp.window_size', 'tcp.window_size_scalefactor', 'tcp.checksum', 'tcp.urgent_pointer', 'tcp.options', 'tcp.options.nop', 'tcp.option_kind', 'tcp.options.timestamp', 'tcp.option_len', 'tcp.options.timestamp.tsval', 'tcp.options.timestamp.tsecr', 'tcp.options.mss', 'tcp.options.mss_val', 'tcp.options.wscale', 'tcp.options.wscale.shift', 'tcp.options.wscale.multiplier', 'tcp.options.sack_perm', 'tls.record.content_type', 'tls.record.version', 'tls.record.length', 'tls.handshake.type', 'tls.handshake.length', 'tls.handshake.version', 'tls.handshake.random_time', 'tls.handshake.random_bytes', 'tls.handshake.session_id_length', 'tls.handshake.cipher_suites_length', 'tls.handshake.ciphersuite', 'tls.handshake.comp_methods_length', 'tls.handshake.comp_method', 'tls.handshake.extensions_length', 'tls.handshake.extension.type', 'tls.handshake.extension.len', 'tls.handshake.extensions_ec_point_formats_length', 'tls.handshake.extensions_ec_point_format', 'tls.handshake.extensions_server_name_list_len', 'tls.handshake.extensions_server_name_type', 'tls.handshake.extensions_server_name_len', 'tls.handshake.extensions_server_name', 'tls.handshake.extensions_reneg_info_len', 'tls.handshake.extensions.supported_versions_len', 'tls.handshake.extensions.supported_version', 'tls.extension.psk_ke_modes_length', 'tls.extension.psk_ke_mode', 'tls.handshake.extensions_key_share_client_length', 'tls.handshake.extensions_key_share_group', 'tls.handshake.extensions_key_share_key_exchange_length', 'tls.handshake.extensions_key_share_key_exchange', 'tls.ech.client_hello_type', 'tls.ech.hpke.keyconfig.cipher_suite.kdf_id', 'tls.ech.hpke.keyconfig.cipher_suite.aead_id', 'tls.ech.config_id', 'tls.ech.enc_length', 'tls.ech.payload_length', 'tls.handshake.extensions_status_request_type', 'tls.handshake.extensions_status_request_responder_ids_len', 'tls.handshake.extensions_status_request_exts_len', 'tls.compress_certificate.algorithms_length', 'tls.compress_certificate.algorithm', 'tls.handshake.extensions_alpn_len', 'tls.handshake.extensions_alpn_str_len', 'tls.handshake.extensions_alpn_str', 'tls.handshake.sig_hash_alg_len', 'tls.handshake.sig_hash_alg', 'tls.handshake.sig_hash_hash', 'tls.handshake.sig_hash_sig', 'tls.handshake.extensions_supported_groups_length', 'tls.handshake.extensions_supported_group', 'tls.record.opaque_type', 'tcp.options.sack', 'tcp.options.sack_le', 'tcp.options.sack_re', 'tls.handshake.certificates_length', 'tls.handshake.certificate_length', 'tls.x509af.version', 'tls.x509af.serialNumber', 'tls.x509af.algorithm.id', 'tls.x509if.RDNSequence_item', 'tls.x509if.oid', 'tls.x509sat.CountryName', 'tls.x509sat.DirectoryString', 'tls.x509sat.printableString', 'tls.x509af.notBefore', 'tls.x509af.utcTime', 'tls.x509af.notAfter', 'tls.pkcs1.publicExponent', 'tls.x509af.extension.id', 'tls.x509af.critical', 'tls.ber.bitstring.padding', 'tls.x509ce.KeyUsage', 'tls.x509ce.KeyUsage.digitalSignature', 'tls.x509ce.KeyUsage.contentCommitment', 'tls.x509ce.KeyUsage.keyEncipherment', 'tls.x509ce.KeyUsage.dataEncipherment', 'tls.x509ce.KeyUsage.keyAgreement', 'tls.x509ce.KeyUsage.keyCertSign', 'tls.x509ce.KeyUsage.cRLSign', 'tls.x509ce.KeyUsage.encipherOnly', 'tls.x509ce.KeyUsage.decipherOnly', 'tls.pkix1implicit.accessMethod', 'tls.x509ce.policyIdentifier', 'tls.x509ce.id', 'tls.x509ce.dNSName', 'tls.x509ce.KeyPurposeIDs', 'tls.x509ce.KeyPurposeId', 'tls.x509ce.keyIdentifier', 'tls.x509ce.SubjectKeyIdentifier', 'tls.sct.scts_length', 'tls.sct.sct_length', 'tls.sct.sct_version', 'tls.sct.sct_timestamp', 'tls.sct.sct_extensions_length', 'tls.sct.sct_signature_length', 'tls.x509ce.cA', 'tls.x509ce.pathLenConstraint', 'tls.handshake.server_curve_type', 'tls.handshake.server_named_curve', 'tls.handshake.server_point_len', 'tls.handshake.sig_len', 'tls.handshake.client_point_len', 'tcp.options.sack.dsack_le', 'tcp.options.sack.dsack_re', 'tls.handshake.session_ticket_lifetime_hint', 'tls.handshake.session_ticket_length', 'eth.padding', 'reassembled_segments']\n"
     ]
    }
   ],
   "source": [
    "print(list_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eth': ['eth.dst', 'eth.src', 'eth.type', 'eth.padding'], 'ip': ['ip.version', 'ip.hdr_len', 'ip.dsfield', 'ip.len', 'ip.id', 'ip.flags', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum', 'ip.src', 'ip.dst'], 'tcp': ['tcp.srcport', 'tcp.dstport', 'tcp.stream', 'tcp.len', 'tcp.seq', 'tcp.seq_raw', 'tcp.ack', 'tcp.ack_raw', 'tcp.hdr_len', 'tcp.flags', 'tcp.window_size_value', 'tcp.window_size', 'tcp.window_size_scalefactor', 'tcp.checksum', 'tcp.urgent_pointer', 'tcp.options', 'tcp.option_kind', 'tcp.option_len'], 'tls': ['tls.record', 'tls.handshake', 'tls.extension', 'tls.ech', 'tls.compress_certificate', 'tls.x509af', 'tls.x509if', 'tls.x509sat', 'tls.pkcs1', 'tls.ber', 'tls.x509ce', 'tls.pkix1implicit', 'tls.sct'], 'statistics': ['frame_num', 'reassembled_segments'], 'eth.dst': ['eth.dst.lg', 'eth.dst.ig'], 'eth.src': ['eth.src.lg', 'eth.src.ig'], 'ip.dsfield': ['ip.dsfield.dscp', 'ip.dsfield.ecn'], 'ip.flags': ['ip.flags.rb', 'ip.flags.df', 'ip.flags.mf'], 'tcp.flags': ['tcp.flags.res', 'tcp.flags.ae', 'tcp.flags.cwr', 'tcp.flags.ece', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.flags.str'], 'tcp.options': ['tcp.options.nop', 'tcp.options.timestamp', 'tcp.options.mss', 'tcp.options.mss_val', 'tcp.options.wscale', 'tcp.options.sack_perm', 'tcp.options.sack', 'tcp.options.sack_le', 'tcp.options.sack_re'], 'tls.record': ['tls.record.content_type', 'tls.record.version', 'tls.record.length', 'tls.record.opaque_type'], 'tls.handshake': ['tls.handshake.type', 'tls.handshake.length', 'tls.handshake.version', 'tls.handshake.random_time', 'tls.handshake.random_bytes', 'tls.handshake.session_id_length', 'tls.handshake.cipher_suites_length', 'tls.handshake.ciphersuite', 'tls.handshake.comp_methods_length', 'tls.handshake.comp_method', 'tls.handshake.extensions_length', 'tls.handshake.extension', 'tls.handshake.extensions_ec_point_formats_length', 'tls.handshake.extensions_ec_point_format', 'tls.handshake.extensions_server_name_list_len', 'tls.handshake.extensions_server_name_type', 'tls.handshake.extensions_server_name_len', 'tls.handshake.extensions_server_name', 'tls.handshake.extensions_reneg_info_len', 'tls.handshake.extensions', 'tls.handshake.extensions_key_share_client_length', 'tls.handshake.extensions_key_share_group', 'tls.handshake.extensions_key_share_key_exchange_length', 'tls.handshake.extensions_key_share_key_exchange', 'tls.handshake.extensions_status_request_type', 'tls.handshake.extensions_status_request_responder_ids_len', 'tls.handshake.extensions_status_request_exts_len', 'tls.handshake.extensions_alpn_len', 'tls.handshake.extensions_alpn_str_len', 'tls.handshake.extensions_alpn_str', 'tls.handshake.sig_hash_alg_len', 'tls.handshake.sig_hash_alg', 'tls.handshake.sig_hash_hash', 'tls.handshake.sig_hash_sig', 'tls.handshake.extensions_supported_groups_length', 'tls.handshake.extensions_supported_group', 'tls.handshake.certificates_length', 'tls.handshake.certificate_length', 'tls.handshake.server_curve_type', 'tls.handshake.server_named_curve', 'tls.handshake.server_point_len', 'tls.handshake.sig_len', 'tls.handshake.client_point_len', 'tls.handshake.session_ticket_lifetime_hint', 'tls.handshake.session_ticket_length'], 'tls.extension': ['tls.extension.psk_ke_modes_length', 'tls.extension.psk_ke_mode'], 'tls.ech': ['tls.ech.client_hello_type', 'tls.ech.hpke', 'tls.ech.config_id', 'tls.ech.enc_length', 'tls.ech.payload_length'], 'tls.compress_certificate': ['tls.compress_certificate.algorithms_length', 'tls.compress_certificate.algorithm'], 'tls.x509af': ['tls.x509af.version', 'tls.x509af.serialNumber', 'tls.x509af.algorithm', 'tls.x509af.notBefore', 'tls.x509af.utcTime', 'tls.x509af.notAfter', 'tls.x509af.extension', 'tls.x509af.critical'], 'tls.x509if': ['tls.x509if.RDNSequence_item', 'tls.x509if.oid'], 'tls.x509sat': ['tls.x509sat.CountryName', 'tls.x509sat.DirectoryString', 'tls.x509sat.printableString'], 'tls.pkcs1': ['tls.pkcs1.publicExponent'], 'tls.ber': ['tls.ber.bitstring'], 'tls.x509ce': ['tls.x509ce.KeyUsage', 'tls.x509ce.policyIdentifier', 'tls.x509ce.id', 'tls.x509ce.dNSName', 'tls.x509ce.KeyPurposeIDs', 'tls.x509ce.KeyPurposeId', 'tls.x509ce.keyIdentifier', 'tls.x509ce.SubjectKeyIdentifier', 'tls.x509ce.cA', 'tls.x509ce.pathLenConstraint'], 'tls.pkix1implicit': ['tls.pkix1implicit.accessMethod'], 'tls.sct': ['tls.sct.scts_length', 'tls.sct.sct_length', 'tls.sct.sct_version', 'tls.sct.sct_timestamp', 'tls.sct.sct_extensions_length', 'tls.sct.sct_signature_length'], 'tcp.options.timestamp': ['tcp.options.timestamp.tsval', 'tcp.options.timestamp.tsecr'], 'tcp.options.wscale': ['tcp.options.wscale.shift', 'tcp.options.wscale.multiplier'], 'tls.handshake.extension': ['tls.handshake.extension.type', 'tls.handshake.extension.len'], 'tls.handshake.extensions': ['tls.handshake.extensions.supported_versions_len', 'tls.handshake.extensions.supported_version'], 'tls.ech.hpke': ['tls.ech.hpke.keyconfig'], 'tls.x509af.algorithm': ['tls.x509af.algorithm.id'], 'tls.x509af.extension': ['tls.x509af.extension.id'], 'tls.ber.bitstring': ['tls.ber.bitstring.padding'], 'tls.x509ce.KeyUsage': ['tls.x509ce.KeyUsage.digitalSignature', 'tls.x509ce.KeyUsage.contentCommitment', 'tls.x509ce.KeyUsage.keyEncipherment', 'tls.x509ce.KeyUsage.dataEncipherment', 'tls.x509ce.KeyUsage.keyAgreement', 'tls.x509ce.KeyUsage.keyCertSign', 'tls.x509ce.KeyUsage.cRLSign', 'tls.x509ce.KeyUsage.encipherOnly', 'tls.x509ce.KeyUsage.decipherOnly'], 'tcp.options.sack': ['tcp.options.sack.dsack_le', 'tcp.options.sack.dsack_re'], 'tls.ech.hpke.keyconfig': ['tls.ech.hpke.keyconfig.cipher_suite'], 'tls.ech.hpke.keyconfig.cipher_suite': ['tls.ech.hpke.keyconfig.cipher_suite.kdf_id', 'tls.ech.hpke.keyconfig.cipher_suite.aead_id']}\n"
     ]
    }
   ],
   "source": [
    "dict_tree = protocol_tree(list_col) \n",
    "print(dict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eth': ['eth.dst', 'eth.src', 'eth.type'], 'ip': ['ip.version', 'ip.hdr_len', 'ip.dsfield', 'ip.len', 'ip.id', 'ip.flags', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum', 'ip.src', 'ip.dst'], 'tcp': ['tcp.srcport', 'tcp.dstport', 'tcp.stream', 'tcp.len', 'tcp.seq', 'tcp.seq_raw', 'tcp.ack', 'tcp.ack_raw', 'tcp.hdr_len', 'tcp.flags', 'tcp.window_size_value', 'tcp.window_size', 'tcp.checksum', 'tcp.urgent_pointer'], 'tls': [], 'statistics': ['frame_num', 'reassembled_segments'], 'eth.dst': ['eth.dst.lg', 'eth.dst.ig'], 'eth.src': ['eth.src.lg', 'eth.src.ig'], 'ip.dsfield': ['ip.dsfield.dscp', 'ip.dsfield.ecn'], 'ip.flags': ['ip.flags.rb', 'ip.flags.df', 'ip.flags.mf'], 'tcp.flags': ['tcp.flags.res', 'tcp.flags.ae', 'tcp.flags.cwr', 'tcp.flags.ece', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.flags.str']}\n"
     ]
    }
   ],
   "source": [
    "dict_tree = protocol_tree(list_col_block) \n",
    "print(dict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eth': [], 'ip': [], 'tcp': [], 'tls': ['tls.handshake'], 'statistics': ['frame_num'], 'tls.handshake': ['tls.handshake.version', 'tls.handshake.session_id_length', 'tls.handshake.ciphersuite', 'tls.handshake.comp_method', 'tls.handshake.extensions_length', 'tls.handshake.extension'], 'tls.handshake.extension': ['tls.handshake.extension.type', 'tls.handshake.extension.len']}\n"
     ]
    }
   ],
   "source": [
    "dict_tree = protocol_tree(list_col_block_tls) \n",
    "print(dict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def create_protocol_tree_graph(data, output_file='protocol_tree'):\n",
    "    # Initialize a directed graph\n",
    "    dot = Digraph(comment='Protocol Tree', format='png')\n",
    "    dot.attr(rankdir='TB')  # Top-to-bottom layout\n",
    "    dot.attr('node', shape='ellipse')  # Default node shape\n",
    "    dot.attr('edge', arrowhead='vee')\n",
    "\n",
    "    # Root node\n",
    "    root_id = 'root'\n",
    "    dot.node(root_id, 'Protocols', shape='box', style='filled', fillcolor='lightblue')\n",
    "\n",
    "    # Function to add nodes and edges recursively\n",
    "    def add_nodes(key, parent_id, prefix=''):\n",
    "        # Create a unique node ID\n",
    "        node_id = f\"{parent_id}_{key.replace('.', '_')}\"\n",
    "        label = key.split('.')[-1] if '.' in key else key  # Show only the last part for brevity\n",
    "        dot.node(node_id, label)\n",
    "\n",
    "        # Add edge from parent to this node\n",
    "        dot.edge(parent_id, node_id)\n",
    "\n",
    "        # If key has children (fields or nested keys)\n",
    "        if key in data:\n",
    "            for field in data[key]:\n",
    "                # Determine if field is a nested key\n",
    "                field_parts = field.split('.')\n",
    "                field_key = '.'.join(field_parts[:len(prefix.split('.')) + 1 if prefix else 1])\n",
    "                \n",
    "                if field_key in data:\n",
    "                    # Nested key, recurse\n",
    "                    add_nodes(field_key, node_id, field)\n",
    "                else:\n",
    "                    # Leaf node (field)\n",
    "                    field_id = f\"{node_id}_{field.replace('.', '_')}\"\n",
    "                    field_label = field.split('.')[-1]  # Show only the last part\n",
    "                    dot.node(field_id, field_label, shape='plaintext')\n",
    "                    dot.edge(node_id, field_id)\n",
    "\n",
    "    # Add top-level keys (eth, ip, tcp, tls, statistics)\n",
    "    for key in ['eth', 'ip', 'tcp', 'tls', 'statistics']:\n",
    "        if key in data:\n",
    "            add_nodes(key, root_id)\n",
    "\n",
    "    # Render and save the graph\n",
    "    dot.render(output_file, view=True, cleanup=True)\n",
    "    return dot \n",
    "\n",
    "create_protocol_tree_graph(dict_tree, 'protocol_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicatedNodeIdError",
     "evalue": "Can't create node with ID 'root_eth_eth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicatedNodeIdError\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m     tree\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\n\u001b[1;32m---> 41\u001b[0m \u001b[43mcreate_protocol_tree_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mcreate_protocol_tree_text\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatistics\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m---> 35\u001b[0m         \u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Display the tree\u001b[39;00m\n\u001b[0;32m     38\u001b[0m tree\u001b[38;5;241m.\u001b[39mshow()\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mcreate_protocol_tree_text.<locals>.add_nodes\u001b[1;34m(key, parent_id, prefix)\u001b[0m\n\u001b[0;32m     21\u001b[0m field_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(field_parts[:\u001b[38;5;28mlen\u001b[39m(prefix\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field_key \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Nested key, recurse\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Leaf node (field)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     field_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mcreate_protocol_tree_text.<locals>.add_nodes\u001b[1;34m(key, parent_id, prefix)\u001b[0m\n\u001b[0;32m     12\u001b[0m node_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m label \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m key  \u001b[38;5;66;03m# Show only the last part\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# If key has children\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[1;32md:\\Tools\\Developer\\Python\\Anaconda\\envs\\Pytorch_envs\\lib\\site-packages\\treelib\\tree.py:437\u001b[0m, in \u001b[0;36mTree.create_node\u001b[1;34m(self, tag, identifier, parent, data)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mCreate a child node for given @parent node. If ``identifier`` is absent,\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03ma UUID will be generated automatically.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_class(tag\u001b[38;5;241m=\u001b[39mtag, identifier\u001b[38;5;241m=\u001b[39midentifier, data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Tools\\Developer\\Python\\Anaconda\\envs\\Pytorch_envs\\lib\\site-packages\\treelib\\tree.py:351\u001b[0m, in \u001b[0;36mTree.add_node\u001b[1;34m(self, node, parent)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst parameter must be object of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_class)\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39midentifier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DuplicatedNodeIdError(\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create node \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith ID \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m node\u001b[38;5;241m.\u001b[39midentifier\n\u001b[0;32m    353\u001b[0m     )\n\u001b[0;32m    355\u001b[0m pid \u001b[38;5;241m=\u001b[39m parent\u001b[38;5;241m.\u001b[39midentifier \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parent, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_class) \u001b[38;5;28;01melse\u001b[39;00m parent\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mDuplicatedNodeIdError\u001b[0m: Can't create node with ID 'root_eth_eth'"
     ]
    }
   ],
   "source": [
    "from treelib import Node, Tree\n",
    "\n",
    "\n",
    "def create_protocol_tree_text(data):\n",
    "    # Initialize a tree\n",
    "    tree = Tree()\n",
    "    tree.create_node(\"Protocols\", \"root\")  # Root node\n",
    "\n",
    "    # Function to add nodes recursively\n",
    "    def add_nodes(key, parent_id, prefix=''):\n",
    "        # Add the current key as a node\n",
    "        node_id = f\"{parent_id}_{key.replace('.', '_')}\"\n",
    "        label = key.split('.')[-1] if '.' in key else key  # Show only the last part\n",
    "        tree.create_node(label, node_id, parent=parent_id)\n",
    "\n",
    "        # If key has children\n",
    "        if key in data:\n",
    "            for field in data[key]:\n",
    "                # Determine if field is a nested key\n",
    "                field_parts = field.split('.')\n",
    "                field_key = '.'.join(field_parts[:len(prefix.split('.')) + 1 if prefix else 1])\n",
    "                \n",
    "                if field_key in data:\n",
    "                    # Nested key, recurse\n",
    "                    add_nodes(field_key, node_id, field)\n",
    "                else:\n",
    "                    # Leaf node (field)\n",
    "                    field_id = f\"{node_id}_{field.replace('.', '_')}\"\n",
    "                    field_label = field.split('.')[-1]  # Show only the last part\n",
    "                    tree.create_node(field_label, field_id, parent=node_id)\n",
    "\n",
    "    # Add top-level keys\n",
    "    for key in ['eth', 'ip', 'tcp', 'tls', 'statistics']:\n",
    "        if key in data:\n",
    "            add_nodes(key, \"root\")\n",
    "\n",
    "    # Display the tree\n",
    "    tree.show()\n",
    "    return tree\n",
    "\n",
    "create_protocol_tree_text(dict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame_num']\n",
      "['eth', 'dst', 'ig']\n",
      "['eth', 'dst']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "test = 'frame_num' \n",
    "test1 = 'eth.dst.ig' \n",
    "test2 = 'eth.dst'\n",
    "a = test.split('.') \n",
    "b = test1.split('.') \n",
    "c = test2.split('.')\n",
    "print(a) \n",
    "print(b)\n",
    "print(c)\n",
    "print(c in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_tree_extract(list_fields: list, list_layers = ['eth', 'ip', 'tcp', 'tls']): \n",
    "    dict_fields = {} \n",
    "    for layer in list_layers: \n",
    "        for field in list_fields: \n",
    "\n",
    "    for field in list_fields: \n",
    "        s_field = field.split('.') \n",
    "        if len(s_field) == 1 and s_field[0] not in list_layers: # statistical information, like frame_num, reassembled_info \n",
    "            dict_fields['statistics'].append(s_field) \n",
    "            \n",
    "        if s_field[0] in list_layers: # layer prefix \n",
    "            if s_field[0] not in dict_fields.keys(): \n",
    "                last_len = len(s_field) \n",
    "                dict_fields[s_field[0]].append(s_field) \n",
    "            \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_cols = df.columns[df.isna().all()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonan = df.drop(columns=all_nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_directory = './Data/Test/filter_nan' \n",
    "# path_tls_csv = os.path.join(path_test_directory, 'tls_test_01.pcapng.csv') \n",
    "# path_tls_reassemble_csv = os.path.join(path_test_directory, 'reassemble_tls_test_01.pcapng.csv') \n",
    "path_test_csv = os.path.join(path_test_directory, 'filter_nan_merge_tls_test_01.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1959, 190)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_test_csv, index_col='frame_num') \n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df = df.notnull() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_true = {} \n",
    "for col_num in range(mask_df.shape[1]): \n",
    "    list_true_indices = list(mask_df[mask_df.iloc[:, col_num]].index) \n",
    "    dict_true[col_num] = list_true_indices  \n",
    "\n",
    "# for col in mask_df.columns: \n",
    "#     list_true_indices = list(mask_df[mask_df.loc[:, col]].index) \n",
    "#     dict_true[col] = list_true_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous\n",
    "dict_block = {\n",
    "    'block': [], \n",
    "    'columns': [], \n",
    "    'rows': [] \n",
    "} \n",
    "\n",
    "block_flag = 0 \n",
    "last_key = 0\n",
    "last_value = dict_true[last_key] \n",
    "list_col = []\n",
    "for key, value in dict_true.items(): \n",
    "    if key == last_key: # init \n",
    "        dict_block['block'].append(block_flag) \n",
    "        list_col.append(key) \n",
    "        dict_block['columns'].append(list_col.copy()) \n",
    "        dict_block['rows'].append(dict_true[key]) \n",
    "    else: \n",
    "        if value == last_value: \n",
    "            dict_block['columns'][block_flag].append(key) \n",
    "        if value != last_value: \n",
    "            block_flag += 1 \n",
    "            dict_block['block'].append(block_flag) \n",
    "            list_col.clear() \n",
    "            list_col.append(key) \n",
    "            dict_block['columns'].append(list_col.copy()) \n",
    "            dict_block['rows'].append(dict_true[key]) \n",
    "            last_key = key \n",
    "            last_value =value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block = pd.DataFrame(dict_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:00, 826.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "block_values = dict_block['block'] \n",
    "columns_values = dict_block['columns'] \n",
    "rows_values = dict_block['rows'] \n",
    "\n",
    "list_sub_df = []\n",
    "\n",
    "for block_name, columns, rows in tqdm(zip(block_values, columns_values, rows_values)): \n",
    "    subset_rows = df.loc[rows]\n",
    "    sub_df = subset_rows.iloc[:, columns]\n",
    "    list_sub_df.append(sub_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_block = {\n",
    "    'block': [], \n",
    "    'columns': [], \n",
    "    'rows': [] \n",
    "} \n",
    "\n",
    "block_flag = 0 \n",
    "list_col = [] \n",
    "list_record_col = []\n",
    "for key, value in dict_true.items(): \n",
    "    if key not in list_record_col: \n",
    "        for ik, iv in dict_true.items(): \n",
    "            if iv == value: \n",
    "                if block_flag not in dict_block['block']: \n",
    "                    dict_block['block'].append(block_flag) \n",
    "                    dict_block['rows'].append(dict_true[key]) \n",
    "                list_col.append(ik) \n",
    "            if iv != value: \n",
    "                continue \n",
    "        dict_block['columns'].append(list_col.copy()) \n",
    "        list_record_col.extend(list_col.copy()) \n",
    "        list_col.clear()\n",
    "        block_flag += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block = pd.DataFrame(dict_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_indices = list(mask_df[mask_df['tls_handshake_server_point_len']].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intesection_series = mask_df.all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
