{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from utils.dataframe_tools import filter_out_nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/Test/merge_tls_test_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def protocol_tree(list_fields: list, list_layers = ['eth', 'ip', 'tcp', 'tls']): \n",
    "#     \"\"\"\n",
    "#     Find the hierarchy structure of protocols by handling the csv columns. \n",
    "#     \"\"\"\n",
    "#     dict_protocol_tree = {item: [] for item in list_layers} \n",
    "#     dict_protocol_tree['statistics'] = [] \n",
    "#     len_prefix = 1 # length of current prefix, i.e. eth\n",
    "#     for field in list_fields: # handle statistic information  \n",
    "#         part_field = field.split('.') \n",
    "#         len_field = len(part_field) \n",
    "#         prefix = part_field[0] # layer: eth, ip, tcp, tls\n",
    "#         if prefix not in list_layers and len_field == 1: \n",
    "#             dict_protocol_tree['statistics'].append(field) \n",
    "#         elif field.startswith(prefix) and len_field - len_prefix == 1: # find fields like xx.xx and input as values\n",
    "#             dict_protocol_tree[prefix].append(field) \n",
    "#     len_prefix += 1 # xx.xx \n",
    "#     list_prefix = [] # xx.xx\n",
    "#     for layer in list_layers: \n",
    "#         # list_prefix = [] # xx.xx\n",
    "#         for prefix in dict_protocol_tree[layer]: # {layer: [prefix]}, find these keys already in the dict \n",
    "#             for field in list_fields: \n",
    "#                 if field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: # find fields like xx.xx.xx and input as values\n",
    "#                     if prefix not in dict_protocol_tree: \n",
    "#                         dict_protocol_tree[prefix] = [] \n",
    "#                         dict_protocol_tree[prefix].append(field)\n",
    "#                     else: \n",
    "#                         dict_protocol_tree[prefix].append(field) \n",
    "#             list_prefix.append(prefix) \n",
    "#     len_prefix += 1 # xx.xx\n",
    "#     for new_layer in list_prefix: # [new_layer, ...] \n",
    "#         if new_layer in dict_protocol_tree:\n",
    "#             for prefix in dict_protocol_tree[new_layer]: # {new_layer: [prefix]}\n",
    "#                 for field in list_fields: # scan the whole column list to find the fields which statify the prefix \n",
    "#                     if field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "#                         if prefix not in dict_protocol_tree: \n",
    "#                             dict_protocol_tree[prefix] = [] \n",
    "#                             dict_protocol_tree[prefix].append(field)\n",
    "#                         else: \n",
    "#                             dict_protocol_tree[prefix].append(field) \n",
    "#     return dict_protocol_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fields_by_prefix(layers: list, layer_prefixes: dict, fields: list, len_prefix: int, init: bool): \n",
    "    next_layers = []\n",
    "    if len(layers) > 0: \n",
    "        if init: \n",
    "            for field in fields: \n",
    "                prefix = field.split('.')[0] \n",
    "                if prefix not in layers and len(field.split('.')) - len_prefix == 0: \n",
    "                    layer_prefixes['statistics'].append(field) \n",
    "                elif field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "                    layer_prefixes[prefix].append(field) \n",
    "                    next_layers.append(field) \n",
    "        else: \n",
    "            for prefix in layers: \n",
    "                # for prefix in layer_prefixes[layer]: \n",
    "                for field in fields: \n",
    "                    if field.startswith(prefix) and len(field.split('.')) - len_prefix == 1: \n",
    "                        if prefix not in layer_prefixes: \n",
    "                            layer_prefixes[prefix] = [] \n",
    "                            layer_prefixes[prefix].append(field) \n",
    "                        else: \n",
    "                            layer_prefixes[prefix].append(field) \n",
    "                    else: \n",
    "                        continue \n",
    "                    next_layers.append(field) \n",
    "            # len_prefix += 1 \n",
    "    init = False\n",
    "    return layer_prefixes, next_layers, init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_tree(list_fields: list, list_layers = ['eth', 'ip', 'tcp', 'tls']): \n",
    "    \"\"\"\n",
    "    Find the hierarchy structure of protocols by handling the csv columns. \n",
    "    \"\"\"\n",
    "    dict_protocol_tree = {item: [] for item in list_layers} \n",
    "    dict_protocol_tree['statistics'] = [] \n",
    "    lens = [len(item.split('.')) for item in list_fields]\n",
    "    len_prefix = 1 # length of current prefix, i.e. eth \n",
    "    max_field_len = max(lens)\n",
    "    init = True\n",
    "    while len_prefix < max_field_len: \n",
    "        # len_prefix += 1 # xx.xx \n",
    "        # list_prefix = [] # xx.xx \n",
    "        dict_protocol_tree, list_layers, init = find_fields_by_prefix(list_layers, dict_protocol_tree, list_fields, len_prefix, init) \n",
    "        len_prefix += 1\n",
    "    return dict_protocol_tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eth': ['eth.dst', 'eth.src', 'eth.type', 'eth.padding'], 'ip': ['ip.version', 'ip.hdr_len', 'ip.dsfield', 'ip.len', 'ip.id', 'ip.flags', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum', 'ip.src', 'ip.dst'], 'tcp': ['tcp.srcport', 'tcp.dstport', 'tcp.stream', 'tcp.len', 'tcp.seq', 'tcp.seq_raw', 'tcp.ack', 'tcp.ack_raw', 'tcp.hdr_len', 'tcp.flags', 'tcp.window_size_value', 'tcp.window_size', 'tcp.window_size_scalefactor', 'tcp.checksum', 'tcp.urgent_pointer', 'tcp.options', 'tcp.option_kind', 'tcp.option_len'], 'tls': ['tls.'], 'statistics': ['frame_num', 'reassembled_segments'], 'eth.dst': ['eth.dst.lg', 'eth.dst.ig'], 'eth.src': ['eth.src.lg', 'eth.src.ig'], 'ip.dsfield': ['ip.dsfield.dscp', 'ip.dsfield.ecn'], 'ip.flags': ['ip.flags.rb', 'ip.flags.df', 'ip.flags.mf'], 'tcp.flags': ['tcp.flags.res', 'tcp.flags.ae', 'tcp.flags.cwr', 'tcp.flags.ece', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.flags.str'], 'tcp.options': ['tcp.options.nop', 'tcp.options.timestamp', 'tcp.options.mss', 'tcp.options.mss_val', 'tcp.options.wscale', 'tcp.options.sack_perm', 'tcp.options.sack', 'tcp.options.sack_le', 'tcp.options.sack_re'], 'tls.': ['tls.record.content_type', 'tls.record.version', 'tls.record.length', 'tls.handshake.type', 'tls.handshake.length', 'tls.handshake.version', 'tls.handshake.random_time', 'tls.handshake.random_bytes', 'tls.handshake.session_id_length', 'tls.handshake.cipher_suites_length', 'tls.handshake.ciphersuite', 'tls.handshake.comp_methods_length', 'tls.handshake.comp_method', 'tls.handshake.extensions_length', 'tls.handshake.extensions_ec_point_formats_length', 'tls.handshake.extensions_ec_point_format', 'tls.handshake.extensions_server_name_list_len', 'tls.handshake.extensions_server_name_type', 'tls.handshake.extensions_server_name_len', 'tls.handshake.extensions_server_name', 'tls.handshake.extensions_reneg_info_len', 'tls.extension.psk_ke_modes_length', 'tls.extension.psk_ke_mode', 'tls.handshake.extensions_key_share_client_length', 'tls.handshake.extensions_key_share_group', 'tls.handshake.extensions_key_share_key_exchange_length', 'tls.handshake.extensions_key_share_key_exchange', 'tls.ech.client_hello_type', 'tls.ech.config_id', 'tls.ech.enc_length', 'tls.ech.payload_length', 'tls.handshake.extensions_status_request_type', 'tls.handshake.extensions_status_request_responder_ids_len', 'tls.handshake.extensions_status_request_exts_len', 'tls.compress_certificate.algorithms_length', 'tls.compress_certificate.algorithm', 'tls.handshake.extensions_alpn_len', 'tls.handshake.extensions_alpn_str_len', 'tls.handshake.extensions_alpn_str', 'tls.handshake.sig_hash_alg_len', 'tls.handshake.sig_hash_alg', 'tls.handshake.sig_hash_hash', 'tls.handshake.sig_hash_sig', 'tls.handshake.extensions_supported_groups_length', 'tls.handshake.extensions_supported_group', 'tls.record.opaque_type', 'tls.handshake.certificates_length', 'tls.handshake.certificate_length', 'tls.x509af.version', 'tls.x509af.serialNumber', 'tls.x509if.RDNSequence_item', 'tls.x509if.oid', 'tls.x509sat.CountryName', 'tls.x509sat.DirectoryString', 'tls.x509sat.printableString', 'tls.x509af.notBefore', 'tls.x509af.utcTime', 'tls.x509af.notAfter', 'tls.pkcs1.publicExponent', 'tls.x509af.critical', 'tls.x509ce.KeyUsage', 'tls.pkix1implicit.accessMethod', 'tls.x509ce.policyIdentifier', 'tls.x509ce.id', 'tls.x509ce.dNSName', 'tls.x509ce.KeyPurposeIDs', 'tls.x509ce.KeyPurposeId', 'tls.x509ce.keyIdentifier', 'tls.x509ce.SubjectKeyIdentifier', 'tls.sct.scts_length', 'tls.sct.sct_length', 'tls.sct.sct_version', 'tls.sct.sct_timestamp', 'tls.sct.sct_extensions_length', 'tls.sct.sct_signature_length', 'tls.x509ce.cA', 'tls.x509ce.pathLenConstraint', 'tls.handshake.server_curve_type', 'tls.handshake.server_named_curve', 'tls.handshake.server_point_len', 'tls.handshake.sig_len', 'tls.handshake.client_point_len', 'tls.handshake.session_ticket_lifetime_hint', 'tls.handshake.session_ticket_length'], 'tcp.options.timestamp': ['tcp.options.timestamp.tsval', 'tcp.options.timestamp.tsecr'], 'tcp.options.wscale': ['tcp.options.wscale.shift', 'tcp.options.wscale.multiplier'], 'tcp.options.sack': ['tcp.options.sack.dsack_le', 'tcp.options.sack.dsack_re'], 'tls.x509ce.KeyUsage': ['tls.x509ce.KeyUsage.digitalSignature', 'tls.x509ce.KeyUsage.contentCommitment', 'tls.x509ce.KeyUsage.keyEncipherment', 'tls.x509ce.KeyUsage.dataEncipherment', 'tls.x509ce.KeyUsage.keyAgreement', 'tls.x509ce.KeyUsage.keyCertSign', 'tls.x509ce.KeyUsage.cRLSign', 'tls.x509ce.KeyUsage.encipherOnly', 'tls.x509ce.KeyUsage.decipherOnly']}\n"
     ]
    }
   ],
   "source": [
    "dict_tree = protocol_tree(list_col) \n",
    "print(dict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eth': ['eth.dst', 'eth.src', 'eth.type', 'eth.padding'], 'ip': ['ip.version', 'ip.hdr_len', 'ip.dsfield', 'ip.len', 'ip.id', 'ip.flags', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum', 'ip.src', 'ip.dst'], 'tcp': ['tcp.srcport', 'tcp.dstport', 'tcp.stream', 'tcp.len', 'tcp.seq', 'tcp.seq_raw', 'tcp.ack', 'tcp.ack_raw', 'tcp.hdr_len', 'tcp.flags', 'tcp.window_size_value', 'tcp.window_size', 'tcp.window_size_scalefactor', 'tcp.checksum', 'tcp.urgent_pointer', 'tcp.options', 'tcp.option_kind', 'tcp.option_len'], 'tls': ['tls.'], 'statistics': ['frame_num', 'reassembled_segments', 'frame_num', 'reassembled_segments', 'frame_num', 'reassembled_segments', 'frame_num', 'reassembled_segments']}\n"
     ]
    }
   ],
   "source": [
    "def find_fields_by_prefix(layers: list, layer_prefixes: dict, fields: list, len_prefix: int, init: bool):\n",
    "    # Initialize next_layers\n",
    "    next_layers = []\n",
    "    \n",
    "    # Ensure 'statistics' exists\n",
    "    if 'statistics' not in layer_prefixes:\n",
    "        layer_prefixes['statistics'] = []\n",
    "    \n",
    "    if len(layers) > 0:\n",
    "        if init:\n",
    "            # Initial phase: Assign fields to top-level layers or statistics\n",
    "            for prefix in layers:\n",
    "                if prefix not in layer_prefixes:\n",
    "                    layer_prefixes[prefix] = []\n",
    "                for field in fields:\n",
    "                    field_parts = field.split('.')\n",
    "                    if not field.startswith(prefix + '.') and len(field_parts) == len_prefix:\n",
    "                        layer_prefixes['statistics'].append(field)\n",
    "                    elif field.startswith(prefix + '.') and len(field_parts) == len_prefix + 1:\n",
    "                        layer_prefixes[prefix].append(field)\n",
    "                        # Add the next prefix to next_layers (e.g., 'dst' from 'eth.dst')\n",
    "                        next_prefix = field_parts[len_prefix]\n",
    "                        if next_prefix not in next_layers:\n",
    "                            next_layers.append(next_prefix)\n",
    "        else:\n",
    "            # Non-initial phase: Process deeper layers\n",
    "            for layer in layers:\n",
    "                if layer not in layer_prefixes:\n",
    "                    continue\n",
    "                for prefix in layer_prefixes[layer]:\n",
    "                    for field in fields:\n",
    "                        # Check if field starts with prefix and has the correct depth\n",
    "                        if field.startswith(prefix + '.') and len(field.split('.')) == len_prefix + 1:\n",
    "                            field_parts = field.split('.')\n",
    "                            next_prefix = field_parts[len_prefix]\n",
    "                            if next_prefix not in layer_prefixes:\n",
    "                                layer_prefixes[next_prefix] = []\n",
    "                            layer_prefixes[next_prefix].append(field)\n",
    "                            if next_prefix not in next_layers:\n",
    "                                next_layers.append(next_prefix)\n",
    "        len_prefix += 1\n",
    "    else:\n",
    "        # If layers is empty, ensure loop can exit\n",
    "        len_prefix += 1\n",
    "    \n",
    "    init = False\n",
    "    return layer_prefixes, len_prefix, next_layers, init\n",
    "\n",
    "def protocol_tree(list_fields: list, list_layers=['eth', 'ip', 'tcp', 'tls']):\n",
    "    \"\"\"\n",
    "    Find the hierarchy structure of protocols by handling the csv columns.\n",
    "    \"\"\"\n",
    "    if not list_fields:\n",
    "        return {'statistics': [], **{layer: [] for layer in list_layers}}\n",
    "\n",
    "    # Initialize dictionary\n",
    "    dict_protocol_tree = {item: [] for item in list_layers}\n",
    "    dict_protocol_tree['statistics'] = []\n",
    "    \n",
    "    # Calculate max field depth\n",
    "    lens = [len(item.split('.')) for item in list_fields]\n",
    "    max_field_len = max(lens, default=1)\n",
    "    \n",
    "    len_prefix = 1  # Start with top-level prefixes (e.g., 'eth')\n",
    "    init = True\n",
    "    list_current_layers = list_layers\n",
    "    \n",
    "    while len_prefix <= max_field_len and list_current_layers:\n",
    "        dict_protocol_tree, len_prefix, next_layers, init = find_fields_by_prefix(\n",
    "            list_current_layers, dict_protocol_tree, list_fields, len_prefix, init\n",
    "        )\n",
    "        list_current_layers = next_layers  # Update layers for next iteration\n",
    "    \n",
    "    return dict_protocol_tree\n",
    "\n",
    "# Test the function\n",
    "dict_tree = protocol_tree(list_col)\n",
    "print(dict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame_num']\n",
      "['eth', 'dst', 'ig']\n",
      "['eth', 'dst']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "test = 'frame_num' \n",
    "test1 = 'eth.dst.ig' \n",
    "test2 = 'eth.dst'\n",
    "a = test.split('.') \n",
    "b = test1.split('.') \n",
    "c = test2.split('.')\n",
    "print(a) \n",
    "print(b)\n",
    "print(c)\n",
    "print(c in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_tree_extract(list_fields: list, list_layers = ['eth', 'ip', 'tcp', 'tls']): \n",
    "    dict_fields = {} \n",
    "    for layer in list_layers: \n",
    "        for field in list_fields: \n",
    "\n",
    "    for field in list_fields: \n",
    "        s_field = field.split('.') \n",
    "        if len(s_field) == 1 and s_field[0] not in list_layers: # statistical information, like frame_num, reassembled_info \n",
    "            dict_fields['statistics'].append(s_field) \n",
    "            \n",
    "        if s_field[0] in list_layers: # layer prefix \n",
    "            if s_field[0] not in dict_fields.keys(): \n",
    "                last_len = len(s_field) \n",
    "                dict_fields[s_field[0]].append(s_field) \n",
    "            \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_cols = df.columns[df.isna().all()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonan = df.drop(columns=all_nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_directory = './Data/Test/filter_nan' \n",
    "# path_tls_csv = os.path.join(path_test_directory, 'tls_test_01.pcapng.csv') \n",
    "# path_tls_reassemble_csv = os.path.join(path_test_directory, 'reassemble_tls_test_01.pcapng.csv') \n",
    "path_test_csv = os.path.join(path_test_directory, 'filter_nan_merge_tls_test_01.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1959, 190)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_test_csv, index_col='frame_num') \n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df = df.notnull() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_true = {} \n",
    "for col_num in range(mask_df.shape[1]): \n",
    "    list_true_indices = list(mask_df[mask_df.iloc[:, col_num]].index) \n",
    "    dict_true[col_num] = list_true_indices  \n",
    "\n",
    "# for col in mask_df.columns: \n",
    "#     list_true_indices = list(mask_df[mask_df.loc[:, col]].index) \n",
    "#     dict_true[col] = list_true_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous\n",
    "dict_block = {\n",
    "    'block': [], \n",
    "    'columns': [], \n",
    "    'rows': [] \n",
    "} \n",
    "\n",
    "block_flag = 0 \n",
    "last_key = 0\n",
    "last_value = dict_true[last_key] \n",
    "list_col = []\n",
    "for key, value in dict_true.items(): \n",
    "    if key == last_key: # init \n",
    "        dict_block['block'].append(block_flag) \n",
    "        list_col.append(key) \n",
    "        dict_block['columns'].append(list_col.copy()) \n",
    "        dict_block['rows'].append(dict_true[key]) \n",
    "    else: \n",
    "        if value == last_value: \n",
    "            dict_block['columns'][block_flag].append(key) \n",
    "        if value != last_value: \n",
    "            block_flag += 1 \n",
    "            dict_block['block'].append(block_flag) \n",
    "            list_col.clear() \n",
    "            list_col.append(key) \n",
    "            dict_block['columns'].append(list_col.copy()) \n",
    "            dict_block['rows'].append(dict_true[key]) \n",
    "            last_key = key \n",
    "            last_value =value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block = pd.DataFrame(dict_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:00, 826.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "block_values = dict_block['block'] \n",
    "columns_values = dict_block['columns'] \n",
    "rows_values = dict_block['rows'] \n",
    "\n",
    "list_sub_df = []\n",
    "\n",
    "for block_name, columns, rows in tqdm(zip(block_values, columns_values, rows_values)): \n",
    "    subset_rows = df.loc[rows]\n",
    "    sub_df = subset_rows.iloc[:, columns]\n",
    "    list_sub_df.append(sub_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_block = {\n",
    "    'block': [], \n",
    "    'columns': [], \n",
    "    'rows': [] \n",
    "} \n",
    "\n",
    "block_flag = 0 \n",
    "list_col = [] \n",
    "list_record_col = []\n",
    "for key, value in dict_true.items(): \n",
    "    if key not in list_record_col: \n",
    "        for ik, iv in dict_true.items(): \n",
    "            if iv == value: \n",
    "                if block_flag not in dict_block['block']: \n",
    "                    dict_block['block'].append(block_flag) \n",
    "                    dict_block['rows'].append(dict_true[key]) \n",
    "                list_col.append(ik) \n",
    "            if iv != value: \n",
    "                continue \n",
    "        dict_block['columns'].append(list_col.copy()) \n",
    "        list_record_col.extend(list_col.copy()) \n",
    "        list_col.clear()\n",
    "        block_flag += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block = pd.DataFrame(dict_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_indices = list(mask_df[mask_df['tls_handshake_server_point_len']].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intesection_series = mask_df.all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
